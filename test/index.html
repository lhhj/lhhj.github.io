<!doctype html>
<html lang="en">
<head>
<meta charset="utf‑8">
<title>LiDAR Depth Demo</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>body{margin:0;overflow:hidden}canvas{display:block;width:100%;height:100%}</style>
</head>
<body>
<script type="module">
import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.165/build/three.module.js';

(async () => {
  if (!navigator.xr) {
    alert('WebXR not available – update to iOS 18+ or use a supported browser.');
    return;
  }

  /* 1. Request an immersive‑AR session with depth‑sensing */
  const session = await navigator.xr.requestSession('immersive-ar', {
    requiredFeatures: ['depth-sensing'],
    optionalFeatures: ['hit-test','dom-overlay'],
    depthSensing: {
      usagePreference: ['cpu-optimized','gpu-optimized'],
      dataFormatPreference: ['luminance-alpha','float32']
    }
  });

  /* 2. Basic three.js renderer bound to the XR session */
  const renderer = new THREE.WebGLRenderer({antialias:true,alpha:true});
  renderer.xr.enabled = true;
  document.body.appendChild(renderer.domElement);
  await renderer.xr.setSession(session);

  const scene = new THREE.Scene();
  const camera = new THREE.PerspectiveCamera();

  /* Point‑cloud geometry that we’ll recycle every frame */
  const MAX_PTS = 60_000;          // keep it small for 60 fps
  const geom = new THREE.BufferGeometry();
  geom.setAttribute('position', new THREE.BufferAttribute(new Float32Array(MAX_PTS*3), 3));
  const mat  = new THREE.PointsMaterial({size:0.01});
  const cloud = new THREE.Points(geom,mat);
  scene.add(cloud);

  renderer.setAnimationLoop((time, frame) => {
    if (frame) {
      const viewerPose = frame.getViewerPose(renderer.xr.getReferenceSpace());
      const view = viewerPose.views[0];

      /* 3. Fetch depth buffer for the current view */
      const depth = frame.getDepthInformation(view);
      if (depth) {
        const {data, width, height, rawValueToMeters} = depth;
        /* Turn every Nth pixel into a vertex */
        let i = 0, j = 0, step = 4;
        const pos = geom.attributes.position.array;
        for (let y = 0; y < height && j < MAX_PTS; y += step) {
          for (let x = 0; x < width && j < MAX_PTS; x += step) {
            const idx = (y * width + x) * 2;        // LA format
            const zMeters = data[idx] * rawValueToMeters;
            /* Re‑project to view space */
            const xn = (x / width) * 2 - 1;
            const yn = (y / height) * 2 - 1;
            pos[i++] = xn * zMeters;
            pos[i++] = -yn * zMeters;
            pos[i++] = -zMeters;
            j++;
          }
        }
        geom.setDrawRange(0, j);
        geom.attributes.position.needsUpdate = true;
      }
    }
    renderer.render(scene, camera);
  });
})();
</script>
</body>
</html>
